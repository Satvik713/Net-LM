{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d34067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (4.12.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2024.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfeafd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from embedding_new import PacketEmbedding\n",
    "\n",
    "\n",
    "class PacketLevelEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_len, num_heads, num_layers, dropout):\n",
    "        super(PacketLevelEncoder, self).__init__()\n",
    "\n",
    "        # initialise the embedding layer\n",
    "        self.embedding = PacketEmbedding(\n",
    "            vocab_size, max_len, embed_dim, dropout)\n",
    "\n",
    "        # initialsise the encoder from PyTorch\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            embed_dim, num_heads, embed_dim * 4, dropout)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "\n",
    "        # initialise the mlm and sfbo predictor\n",
    "        self.mlm_predictor = nn.Linear(embed_dim, vocab_size)\n",
    "        self.sfbo_predictor = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 3, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, packet_sequences, field_pos, header_pos):\n",
    "\n",
    "        # splits the seq into masked seqs\n",
    "        masked_packets, span_masks = apply_mlm_sfbo_masking(packet_sequences)\n",
    "        packet_embeddings = self.embedding(masked_packets, field_pos, header_pos)\n",
    "        encoded_packets = self.encoder(packet_embeddings)\n",
    "\n",
    "        mlm_loss, mlm_logits = self.compute_mlm_loss(encoded_packets, masked_packets)\n",
    "        # sfbo_loss = self.compute_sfbo_loss(\n",
    "        #     encoded_packets, span_masks, packet_sequences)\n",
    "        sfbo_loss = 0\n",
    "\n",
    "        return mlm_loss, sfbo_loss, encoded_packets, masked_packets, mlm_logits \n",
    "\n",
    "    def compute_mlm_loss(self, encoded_packets, masked_packets):\n",
    "        mlm_logits = self.mlm_predictor(encoded_packets)\n",
    "        mlm_loss = F.cross_entropy(\n",
    "            mlm_logits.view(-1, mlm_logits.size(-1)), masked_packets.view(-1))\n",
    "        return mlm_loss, mlm_logits\n",
    "\n",
    "    def compute_sfbo_loss(self, encoded_packets, span_masks, packet_sequences):\n",
    "        sfbo_loss = 0\n",
    "        total_spans = 0\n",
    "\n",
    "        for i, (start_tokens, end_tokens, span_tokens) in enumerate(span_masks):\n",
    "            if len(start_tokens) == 0:  # Skip if no spans for this sequence\n",
    "                continue\n",
    "\n",
    "            start_embeddings = encoded_packets[i, start_tokens].view(-1, encoded_packets.size(-1))\n",
    "            end_embeddings = encoded_packets[i, end_tokens].view(-1, encoded_packets.size(-1))\n",
    "            span_embeddings = encoded_packets[i, span_tokens].view(-1, encoded_packets.size(-1))\n",
    "\n",
    "            # Ensure the correct shape\n",
    "            print(\"start_embeddings shape:\", start_embeddings.shape)\n",
    "            print(\"end_embeddings shape:\", end_embeddings.shape)\n",
    "            print(\"span_embeddings shape:\", span_embeddings.shape)\n",
    "\n",
    "            # Concatenate the embeddings along the feature dimension (dim=-1)\n",
    "            span_representations = torch.cat([start_embeddings, end_embeddings, span_embeddings], dim=-1)\n",
    "\n",
    "            # Predict the entire span sequence\n",
    "            sfbo_logits = self.sfbo_predictor(span_representations)\n",
    "\n",
    "            # Flatten logits and targets\n",
    "            flat_logits = sfbo_logits.view(-1, sfbo_logits.size(-1))\n",
    "            flat_targets = packet_sequences[i, span_tokens].view(-1)\n",
    "\n",
    "            # Calculate the loss for each token in the span\n",
    "            sfbo_loss += F.cross_entropy(flat_logits, flat_targets)\n",
    "            total_spans += len(span_tokens)\n",
    "\n",
    "        return sfbo_loss / total_spans if total_spans > 0 else torch.tensor(0.0).to(packet_sequences.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def apply_sfbo_masking(packet_seq, sfbo_prob, max_span_length):\n",
    "    num_tokens = packet_seq.size(0)\n",
    "    sfbo_mask = torch.zeros(num_tokens, dtype=torch.bool)\n",
    "\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    span_indices = []\n",
    "\n",
    "    num_spans = max(1, int(sfbo_prob * num_tokens))\n",
    "    print(\"num spans: \", num_spans)\n",
    "    while num_spans > 0:\n",
    "        start = random.randint(0, num_tokens - 1)\n",
    "        span_length = random.randint(1, max_span_length)\n",
    "        end = min(start + span_length, num_tokens) - 1\n",
    "        if not sfbo_mask[start:end + 1].any():\n",
    "            start_indices.append(start)\n",
    "            end_indices.append(end)\n",
    "            span_indices.extend(range(start, end + 1))\n",
    "            sfbo_mask[start:end + 1] = True\n",
    "            num_spans -= 1\n",
    "    \n",
    "    start_indices_tensor = torch.tensor(start_indices, dtype=torch.long)\n",
    "    end_indices_tensor = torch.tensor(end_indices, dtype=torch.long)\n",
    "    span_indices_tensor = torch.tensor(span_indices, dtype=torch.long)\n",
    "    print(\"Number of spans selected:\", len(start_indices_tensor))\n",
    "\n",
    "    return start_indices_tensor, end_indices_tensor, span_indices_tensor\n",
    "\n",
    "def apply_mlm_sfbo_masking(packet_sequences, mlm_prob=0.15, sfbo_prob=0.15, max_span_length=6):\n",
    "    batch_size, seq_length, _ = packet_sequences.size()\n",
    "    masked_sequences = packet_sequences.clone()\n",
    "\n",
    "    # Apply MLM masking\n",
    "    mlm_mask = torch.rand(batch_size, seq_length) < mlm_prob\n",
    "    masked_sequences[mlm_mask] = torch.tensor(4).to(packet_sequences.device)\n",
    "\n",
    "    # Apply SFBO masking\n",
    "    span_masks = []\n",
    "    for packet_seq in masked_sequences:\n",
    "        span_mask = apply_sfbo_masking(packet_seq, sfbo_prob, max_span_length)\n",
    "        span_masks.append(span_mask)\n",
    "\n",
    "    return masked_sequences, span_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "950f1e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from Input_Tokenizer import Tokenizer\n",
    "from embedding_new import PacketEmbedding, FlowEmbedding\n",
    "# from packet_encoder import PacketLevelEncoder, apply_mlm_sfbo_masking\n",
    "# from flow_encoder import FlowLevelEncoder\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from field_header_pos_encoding import field_pos, header_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f06f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 30000\n",
    "embed_dim = 768\n",
    "num_heads = 12\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "max_flow_length = 510\n",
    "mask_prob = 0.15\n",
    "num_epochs = 10\n",
    "max_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cec1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d87ef65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bd9acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading vocabulary\n",
    "vocab = {}\n",
    "custom_vocab_path = r\"/home/satvik/spark/spark/vocab_1.txt\"\n",
    "with open(custom_vocab_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        token, token_id = line.strip().split('\\t')\n",
    "        vocab[token] = int(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "391c5bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the tokenizer\n",
    "tokenizer = Tokenizer(vocab_file=custom_vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "793f3d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satvik/miniconda3/envs/lama/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize packet embedding and encoding modules\n",
    "packet_embedding = PacketEmbedding(\n",
    "    vocab_size, max_len=512, embed_dim=embed_dim, dropout=dropout).to(device)\n",
    "packet_encoder = PacketLevelEncoder(   \n",
    "    vocab_size, embed_dim, max_len, num_heads, num_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "874fba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(packet_embedding.parameters(\n",
    ")) + list(packet_encoder.parameters()) , lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fbeb83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "\n",
    "class PacketSequenceDataset(Dataset):\n",
    "    def __init__(self, packet_seq_dir, field_pos_dir, header_pos_dir, tokenizer):\n",
    "        self.packet_seq_dir = packet_seq_dir\n",
    "        self.field_pos_dir = field_pos_dir\n",
    "        self.header_pos_dir = header_pos_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.packet_sequences = []\n",
    "        self.field_pos_sequences = []\n",
    "        self.header_pos_sequences = []\n",
    "\n",
    "        # Preprocess data\n",
    "        packet_seq_files = [os.path.join(packet_seq_dir, file) for file in os.listdir(\n",
    "            packet_seq_dir) if file.endswith('.txt')]\n",
    "        field_pos_files = [os.path.join(field_pos_dir, file) for file in os.listdir(\n",
    "            field_pos_dir) if file.endswith('.txt')]\n",
    "        header_pos_files = [os.path.join(header_pos_dir, file) for file in os.listdir(\n",
    "            header_pos_dir) if file.endswith('.txt')]\n",
    "\n",
    "        for packet_seq_file, field_pos_file, header_pos_file in zip(packet_seq_files, field_pos_files, header_pos_files):\n",
    "            with open(packet_seq_file, 'r', encoding='utf-8') as f:\n",
    "                hex_dumps = f.readlines()\n",
    "                padded_all_tokens, token_ids, mask, max_length = self.tokenizer.encode_packet(\n",
    "                    hex_dumps)\n",
    "                self.packet_sequences.append(token_ids.to(device))\n",
    "                print(\"td :\", token_ids.shape)\n",
    "\n",
    "            '''\n",
    "            with open(packet_seq_file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    tokens = self.tokenizer.encode_packet(\n",
    "                        line.strip(), add_special_tokens=True, truncation=True, padding='max_length')\n",
    "                    self.packet_sequences.append(tokens)\n",
    "                '''\n",
    "            field_posn = field_pos(field_pos_file).to(device).long()\n",
    "            print(\"fp: \", field_posn.shape) \n",
    "            self.field_pos_sequences.append(field_posn)\n",
    "\n",
    "            header_posn = header_pos(header_pos_file).to(device).long()\n",
    "            print(\"hp :\", header_posn.shape)\n",
    "            self.header_pos_sequences.append(header_posn)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.packet_sequences)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "        self.packet_sequences[idx],\n",
    "        self.field_pos_sequences[idx],\n",
    "        self.header_pos_sequences[idx]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "466e3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.invalidate_caches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e06cfb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td : torch.Size([4, 469])\n",
      "field: torch.Size([4, 469])\n",
      "fp:  torch.Size([4, 469])\n",
      "header: torch.Size([4, 469])\n",
      "hp : torch.Size([4, 469])\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset and data loader\n",
    "dataset = PacketSequenceDataset(\n",
    "    packet_seq_dir=r'/home/satvik/spark/spark/packets', field_pos_dir='/home/satvik/spark/spark/fields', header_pos_dir='/home/satvik/spark/spark/headers', tokenizer=tokenizer)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22e9cfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "1\n",
      "torch.Size([4, 469])\n",
      "packet seq shape:  torch.Size([1, 4, 469])\n",
      "num spans:  1\n",
      "Number of spans selected: 1\n",
      "token ids : torch.Size([4, 469])\n",
      "num packets:  4\n",
      "seq len:  469\n",
      "torch.Size([4, 469, 768])\n",
      "torch.Size([4, 469, 768])\n",
      "torch.Size([4, 469, 768])\n",
      "torch.Size([4, 469, 768])\n",
      "mlm: tensor(10.4134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "0\n",
      "torch.Size([1, 4, 469])\n",
      "torch.Size([4, 469, 768])\n",
      "torch.Size([4, 469, 30000])\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch 1\")\n",
    "    for packet_sequences, field_pos, header_pos in train_loader:\n",
    "        \n",
    "        # Forward pass\n",
    "        print(\"1\")\n",
    "        field_pos = field_pos.squeeze(0)\n",
    "        header_pos = header_pos.squeeze(0)\n",
    "\n",
    "        # for i in range(4):\n",
    "        #     print(field_pos[i][:])\n",
    "        print(header_pos.shape)\n",
    "        # print(\"packet seq :\", packet_sequences.shape)\n",
    "        # print(packet_sequences)\n",
    "        # packet_sequences = packet_sequences.squeeze(0)\n",
    "        print(\"packet seq shape: \", packet_sequences.shape)\n",
    "\n",
    "        mlm, sfbo, enc, mask, mlm_log = packet_encoder(packet_sequences, field_pos, header_pos)\n",
    "\n",
    "        print('mlm:', mlm)\n",
    "        print(sfbo)\n",
    "        print(mask.shape)\n",
    "        print(enc.shape)\n",
    "        print(mlm_log.shape)\n",
    "        break\n",
    "    break\n",
    "        # packet_seq_cpu = packet_sequences.cpu()\n",
    "        # packet_seq_np = packet_seq_cpu.numpy()\n",
    "\n",
    "        # # Open a file to write the data\n",
    "        # with open('/home/satvik/spark/spark/packet_seq.txt', 'w') as file:\n",
    "        #     for row in packet_seq_np:\n",
    "        #         # Convert each row to a space-separated string and write it to the file\n",
    "        #         row_str = ' '.join(map(str, row))\n",
    "        #         file.write(row_str + '\\n')\n",
    "\n",
    "        # print(\"Tensor data has been written to 'packet_seq.txt'.\")\n",
    "\n",
    "        # Generate masked tokens\n",
    "        # masked_packets, span_masks = apply_mlm_sfbo_masking(packet_sequences)\n",
    "        # print(\"masked packets shape: \", masked_packets.shape)\n",
    "        # print(\"span masks: \", span_masks)\n",
    "\n",
    "        # # moving to same device\n",
    "        # packet_sequences = packet_sequences.to(device)\n",
    "        # fiels_pos = field_pos.to(device)\n",
    "        # header_pos = header_pos.to(device)\n",
    "        # masked_packets = masked_packets.to(device)\n",
    "        \n",
    "        # # compute packet embeddings\n",
    "        # packet_embeddings = packet_embedding(\n",
    "        #     token_ids=masked_packets, field_pos=field_pos, header_pos=header_pos)\n",
    "        # print(\"pe shape: \", packet_embeddings.shape)\n",
    "        \n",
    "        # # calucate loss\n",
    "        # mlm_loss = packet_encoder.compute_mlm_loss(packet_embeddings, masked_packets)\n",
    "        # sfbo_loss = packet_encoder.compute_sfbo_loss(packet_embeddings, span_masks, packet_sequences)\n",
    "        # # flow_encodings, mpm_losses = flow_encoder(flow_sequences)\n",
    "        # print(\"mlm loss: \", mlm_loss.item())\n",
    "        # print(\"sfbo_loss: \", sfbo_loss.item())\n",
    "        # total_loss = mlm_loss  + sfbo_loss\n",
    "        # print(\"total loss: \", total_loss.item())\n",
    "        \n",
    "        # # Compute total loss\n",
    "        # # total_loss = mlm_loss + sfbo_lossD\n",
    "        # ''' + sum(mpm_losses)'''\n",
    "        \n",
    "        # optimizer.zero_grad()\n",
    "        # total_loss.backward()\n",
    "        # optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd95e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
